{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "SCHEDULER = 'Lambda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./processed_train_samples.csv')\n",
    "val_df = pd.read_csv(\"./processed_val_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['cat3'].values)\n",
    "\n",
    "train_df['cat3'] = le.transform(train_df['cat3'].values)\n",
    "val_df['cat3'] = le.transform(val_df['cat3'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vectorizer = CountVectorizer(max_features=4096)\n",
    "tfid_vectorizer = TfidfVectorizer(max_features=4096)\n",
    "# hash_vectorizer = HashingVectorizer(n_features=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_train_vectors = tfid_vectorizer.fit_transform(train_df['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_val_vectors = tfid_vectorizer.transform(val_df['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13588it [00:00, 15723.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for n, i in tqdm(enumerate(tfid_train_vectors.todense())):\n",
    "#     if np.sum(i) == 0.0:\n",
    "#         print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(tfid_train_vectors.toarray(), columns=tfid_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfid_vectorizer.vocabulary_\n",
    "# tfid_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, text_vectors, label_list, infer=False):\n",
    "        self.text_vectors = text_vectors\n",
    "        self.label_list = label_list\n",
    "        self.infer = infer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # NLP\n",
    "        text_vetors = torch.Tensor(self.text_vectors[index]).view(-1)\n",
    "\n",
    "        # Label\n",
    "        if self.infer: # infer == True, test_data로부터 label \"결과 추출\" 시 사용\n",
    "            return text_vetors\n",
    "        else: # infer == False\n",
    "            label = self.label_list[index] # dataframe에서 label 가져와 \"학습\" 시 사용\n",
    "            return text_vetors, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(tfid_train_vectors, train_df['cat3'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=0) # 6\n",
    "\n",
    "val_dataset = CustomDataset(tfid_val_vectors, val_df['cat3'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Image\n",
    "        # Text\n",
    "        self.count_extract = nn.Sequential(\n",
    "            nn.Linear(4096, 2048), # 선형회귀. 4096개의 입력으로 2048개의 출력\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024), # 선형회귀. 2048개의 입력으로 1024개의 출력\n",
    "        )\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, num_classes))\n",
    "            \n",
    "\n",
    "    def forward(self, text1, text2, text3):\n",
    "        text_feature1 = self.count_extract(text1)\n",
    "        output = self.classifier(text_feature1) # classifier 적용\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(real, pred):\n",
    "    return f1_score(real, pred, average=\"weighted\")\n",
    "\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval() # nn.Module에서 train time과 eval time에서 수행하는 다른 작업을 수행할 수 있도록 switching 하는 함수\n",
    "    \n",
    "    model_preds = [] # 예측값\n",
    "    true_labels = [] # 실제값\n",
    "    \n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, label in tqdm(iter(val_loader)): # val_loader에서 img, text, label 가져옴\n",
    "            tfid = text.to(device)\n",
    "            label = label.type(torch.LongTensor) # label type을 LongTensor로 형변환, 추가하여 에러 해결\n",
    "            label = label.to(device)\n",
    "            \n",
    "            model_pred = model(tfid)\n",
    "            \n",
    "            loss = criterion(model_pred, label) # 예측값, 실제값으로 손실함수 적용 -> loss 추출\n",
    "            \n",
    "            val_loss.append(loss.item()) # loss 출력, val_loss에 저장\n",
    "            \n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "        \n",
    "    test_weighted_f1 = score_function(true_labels, model_preds) # 실제 라벨값들과 예측한 라벨값들에 대해 f1 점수 계산\n",
    "    return np.mean(val_loss), test_weighted_f1 # 각각 val_loss, val_score에 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device) # gpu(cpu)에 적용\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device) # CrossEntropyLoss: 다중분류를 위한 손실함수\n",
    "    best_score = 0\n",
    "    best_model = None \n",
    "    dir_name = os.path.join(\"./model_weights/\", f\"ONE_TextVector_4096_scheduler_{SCHEDULER}_batch_{BATCH_SIZE}_lr_{LEARNING_RATE}_{random.randrange(10000,99000)}\")\n",
    "    os.mkdir(dir_name)\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        model.train() \n",
    "        train_loss = []\n",
    "        for text, label in tqdm(iter(train_loader)): # train_loader에서 img, text, label 가져옴\n",
    "            tfid = text.to(device)\n",
    "            label = label.type(torch.LongTensor) \n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # 이전 루프에서 .grad에 저장된 값이 다음 루프의 업데이트에도 간섭하는 걸 방지, 0으로 초기화\n",
    "\n",
    "            model_pred = model(tfid) # 예측\n",
    "            loss = criterion(model_pred, label) # 예측값과 실제값과의 손실 계산\n",
    "\n",
    "            loss.backward() # .backward() 를 호출하면 역전파가 시작\n",
    "            optimizer.step() # optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        # 모든 train_loss 가져옴\n",
    "        tr_loss = np.mean(train_loss)\n",
    "            \n",
    "        val_loss, val_score = validation(model, criterion, val_loader, device) # 검증 시작, 여기서 validation 함수 사용\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            print(f'Epoch [{epoch}], LR: [{scheduler.get_lr()[0]}]Train Loss : [{tr_loss:.5f}] Val Loss : [{val_loss:.5f}] Val Score : [{val_score:.5f}]')\n",
    "        else:\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{tr_loss:.5f}] Val Loss : [{val_loss:.5f}] Val Score : [{val_score:.5f}]')\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            # scheduler의 의미: Learning Rate Scheduler => learning rate를 조절한다. \n",
    "            # DACON에서는 CosineAnnealingLR 또는 CosineAnnealingWarmRestarts 를 주로 사용한다.\n",
    "            \n",
    "        if best_score < val_score: # 최고의 val_score을 가진 모델에 대해서만 최종적용을 시킴\n",
    "            best_score = val_score\n",
    "            best_model = model\n",
    "            torch.save({'epoch': epoch,\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            },\n",
    "            os.path.join(dir_name, f\"ONE_TextVector_epoch_{epoch}_val_acc_{best_score}.pth\"))\n",
    "            \n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CustomModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=False)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, torch.device('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('p39t10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0391b7989f367334aa233f75d4a9cb1090673f0ab48eb1800eba39ea6aecf8f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
